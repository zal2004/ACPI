{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "from typing import Optional, Callable\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_TRAIN_PATH = \"data/train.csv\"\n",
    "DATA_TEST_PATH = \"data/test.csv\"\n",
    "target_col = \"count\"\n",
    "\n",
    "def read_data(path):\n",
    "    df = pd.read_csv(path, parse_dates=[\"datetime\"])\n",
    "\n",
    "    season = pd.get_dummies(df['season'], prefix='season')\n",
    "    df = pd.concat([df, season], axis=1)\n",
    "\n",
    "    weather = pd.get_dummies(df['weather'], prefix='weather')\n",
    "    df = pd.concat([df, weather], axis=1)\n",
    "\n",
    "    df.drop(['season', 'weather'], axis=1, inplace=True)\n",
    "\n",
    "    dt_index = pd.DatetimeIndex(df.datetime)\n",
    "    df['hour'] = dt_index.hour\n",
    "    df['day'] = dt_index.dayofweek\n",
    "    df['month'] = dt_index.month\n",
    "    df['year'] = dt_index.year.map({2011: 0, 2012: 1})\n",
    "\n",
    "    df.drop('datetime', axis=1, inplace=True)\n",
    "\n",
    "    # Drop train-only columns if present\n",
    "    drop_cols = [c for c in ['casual', 'registered'] if c in df.columns]\n",
    "    if drop_cols:\n",
    "        df.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = read_data(DATA_TRAIN_PATH)\n",
    "test_df = read_data(DATA_TEST_PATH)\n",
    "\n",
    "feature_cols = [c for c in train_df.columns if c != target_col and c in test_df.columns]\n",
    "print(f\"Feature count: {len(feature_cols)}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_df[feature_cols].values.astype(np.float32))\n",
    "train_target = train_df[target_col].values.astype(np.float32)\n",
    "\n",
    "\n",
    "def build_lag_supervised(features, target, context_length):\n",
    "    X_list, y_list = [], []\n",
    "    for i in range(len(target) - context_length):\n",
    "        X_window = features[i:i + context_length].reshape(-1)\n",
    "        y_val = target[i + context_length]\n",
    "        X_list.append(X_window)\n",
    "        y_list.append(y_val)\n",
    "    return np.vstack(X_list).astype(np.float32), np.array(y_list, dtype=np.float32)\n",
    "\n",
    "\n",
    "context_length = 24\n",
    "X_all, y_all = build_lag_supervised(train_features, train_target, context_length)\n",
    "print(\"Supervised shape:\", X_all.shape, y_all.shape)\n",
    "\n",
    "# Train / holdout split (time-ordered)\n",
    "train_ratio = 0.8\n",
    "n_train = int(len(X_all) * train_ratio)\n",
    "X_model, X_test_hold = X_all[:n_train], X_all[n_train:]\n",
    "y_model, y_test_hold = y_all[:n_train], y_all[n_train:]\n",
    "print(\"Train segment:\", X_model.shape, \"Test segment:\", X_test_hold.shape)\n",
    "\n",
    "# Train / validation for early stopping\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_model, y_model, test_size=0.15, shuffle=False)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"eta\": 0.05,\n",
    "    \"max_depth\": 8,\n",
    "    \"subsample\": 0.9,\n",
    "    \"colsample_bytree\": 0.9,\n",
    "    \"lambda\": 1.0,\n",
    "    \"gamma\": 0.0,\n",
    "    \"min_child_weight\": 1\n",
    "}\n",
    "\n",
    "evals = [(dtrain, \"train\"), (dval, \"val\")]\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=2000,\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=50,\n",
    "    loss = \"rmse\"\n",
    ")\n",
    "\n",
    "# Evaluate on holdout\n",
    "y_pred = bst.predict(xgb.DMatrix(X_test_hold))\n",
    "\n",
    "rmse = mean_squared_error(y_test_hold, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test_hold, y_pred)\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    mape = np.mean(np.abs((y_test_hold - y_pred) / np.clip(np.abs(y_test_hold), 1e-6, None))) * 100\n",
    "r2 = r2_score(y_test_hold, y_pred)\n",
    "\n",
    "rmsle = np.sqrt(mean_squared_log_error(y_test_hold, y_pred))\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"RMSLE: {rmsle:.4f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "print(f\"R2:   {r2:.4f}\")\n",
    "\n",
    "# Plot predictions vs actual\n",
    "n_plot = min(200, len(y_test_hold))\n",
    "idx = np.arange(n_plot)\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(idx, y_test_hold[:n_plot], label=\"True\", color=\"black\")\n",
    "plt.plot(idx, y_pred[:n_plot], label=\"Pred\", color=\"orange\")\n",
    "plt.title(\"Holdout predictions\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Error distribution\n",
    "errors = y_pred - y_test_hold\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(errors, bins=40, color=\"steelblue\", alpha=0.8)\n",
    "plt.title(\"Prediction error distribution\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Feature importance (by gain)\n",
    "importance = bst.get_score(importance_type=\"gain\")\n",
    "if importance:\n",
    "    imp_items = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "    labels, vals = zip(*imp_items)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.barh(labels[::-1], vals[::-1], color=\"teal\")\n",
    "    plt.title(\"Top feature importance (gain)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No feature importance available.\")\n",
    "\n",
    "print(\"Done.\")\n"
   ],
   "id": "45371cc8b9811942"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ACPI Implementation\n",
    "from acpi import ACPI\n",
    "from acpi.utils import compute_coverage\n",
    "\n",
    "# Split holdout set into calibration and test sets\n",
    "cal_size = 0.5\n",
    "n_cal = int(len(X_test_hold) * cal_size)\n",
    "X_cal, X_test_acpi = X_test_hold[:n_cal], X_test_hold[n_cal:]\n",
    "y_cal, y_test_acpi = y_test_hold[:n_cal], y_test_hold[n_cal:]\n",
    "\n",
    "print(f\"Calibration set size: {len(X_cal)}\")\n",
    "print(f\"ACPI Test set size: {len(X_test_acpi)}\")\n",
    "\n",
    "\n",
    "class XGBoostACPIWrapper:\n",
    "    \"\"\"ACPI wrapper for an XGBoost model.\"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict using the XGBoost model, handling DMatrix conversion.\"\"\"\n",
    "        dmatrix = xgb.DMatrix(X)\n",
    "        return self.model.predict(dmatrix)\n",
    "\n",
    "\n",
    "# Wrap the trained XGBoost model\n",
    "xgb_wrapper = XGBoostACPIWrapper(bst)"
   ],
   "id": "2b76c2a776db1db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize and fit ACPI\n",
    "alpha = 0.1\n",
    "acpi_model = ACPI(\n",
    "    model_cali=xgb_wrapper,\n",
    "    n_estimators=200,\n",
    "    max_depth=25,\n",
    "    min_node_size=15\n",
    ")\n",
    "\n",
    "print(\"\\nFitting ACPI model...\")\n",
    "acpi_model.fit(X_cal, y_cal)\n",
    "acpi_model.fit_calibration(X_cal, y_cal, quantile=(1 - alpha), only_qrf=True)\n",
    "print(\"ACPI fitting complete.\")\n",
    "\n",
    "# Generate prediction intervals\n",
    "y_lower, y_upper = acpi_model.predict_pi(X_test_acpi, method=\"qrf\")\n",
    "\n",
    "# Evaluate coverage\n",
    "coverage = compute_coverage(y_test_acpi, y_lower, y_upper)\n",
    "print(f\"\\nACPI Coverage: {coverage:.4f} (Target: {1-alpha:.4f})\")\n",
    "\n",
    "# Get point predictions for comparison\n",
    "y_pred_acpi = xgb_wrapper.predict(X_test_acpi)"
   ],
   "id": "6ec01c7de1635681"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Plot ACPI Results\n",
   "id": "d34c2897921964ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot predictions with intervals\n",
    "n_plot_acpi = min(200, len(y_test_acpi))\n",
    "idx_acpi = np.arange(n_plot_acpi)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(idx_acpi, y_test_acpi[:n_plot_acpi], label=\"True\", color=\"black\", linewidth=1)\n",
    "plt.plot(idx_acpi, y_pred_acpi[:n_plot_acpi], label=\"XGBoost Mean\", color=\"orange\", linewidth=1)\n",
    "plt.fill_between(idx_acpi, y_lower[:n_plot_acpi], y_upper[:n_plot_acpi], color=\"steelblue\", alpha=0.4, label=\"ACPI Prediction Interval\")\n",
    "plt.title(f\"ACPI Prediction Intervals (Coverage: {coverage:.3f})\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Plot interval widths\n",
    "interval_widths = y_upper - y_lower\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(interval_widths[:n_plot_acpi], label=\"Interval Width\")\n",
    "plt.title(\"ACPI Interval Widths\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Width\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Coverage statistics\n",
    "n_below = (y_test_acpi < y_lower).sum()\n",
    "n_above = (y_test_acpi > y_upper).sum()\n",
    "n_within = len(y_test_acpi) - n_below - n_above\n",
    "total = len(y_test_acpi)\n",
    "\n",
    "print(f\"Below: {n_below} ({n_below/total:.2%})\")\n",
    "print(f\"Above: {n_above} ({n_above/total:.2%})\")\n",
    "print(f\"Within: {n_within} ({n_within/total:.2%})\")"
   ],
   "id": "67a1f31ac034c9f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from itertools import product\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def acpi_grid_search(X_cal, y_cal, X_test, y_test, xgb_wrapper, alpha=0.1,\n",
    "                     param_grid=None):\n",
    "\n",
    "\n",
    "    target_coverage = 1 - alpha\n",
    "    results = []\n",
    "    param_combinations = list(product(*param_grid.values()))\n",
    "    total_combinations = len(param_combinations)\n",
    "    print(f\"Total parameter combinations to evaluate: {total_combinations}\")\n",
    "\n",
    "    for params in tqdm(param_combinations, desc=\"Grid Search\"):\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "\n",
    "        acpi_model = ACPI(\n",
    "            model_cali=xgb_wrapper,\n",
    "            n_estimators=param_dict['n_estimators'],\n",
    "            max_depth=param_dict['max_depth'],\n",
    "            min_node_size=param_dict['min_node_size']\n",
    "        )\n",
    "\n",
    "        acpi_model.fit(X_cal, y_cal)\n",
    "        acpi_model.fit_calibration(X_cal, y_cal, quantile=target_coverage, only_qrf=True)\n",
    "\n",
    "        y_lower, y_upper = acpi_model.predict_pi(X_test, method=\"qrf\")\n",
    "        coverage = compute_coverage(y_test, y_lower, y_upper)\n",
    "\n",
    "        results.append({\n",
    "            **param_dict,\n",
    "            'coverage': coverage,\n",
    "            'coverage_diff': abs(coverage - target_coverage)\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values(by='coverage_diff')\n",
    "    return results_df\n",
    "# Perform grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_depth': [15, 20, 25],\n",
    "    'min_node_size': [10, 15, 20]\n",
    "}\n",
    "\n",
    "grid_results = acpi_grid_search(X_cal, y_cal, X_test_acpi, y_test_acpi, xgb_wrapper, alpha=0.1, param_grid=param_grid)\n",
    "print(\"\\nGrid Search Results:\")\n",
    "print(grid_results.head(10))\n",
    "\n"
   ],
   "id": "f0c88847e9de3cef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# save the grid search results to a CSV file\n",
    "grid_results.to_csv(\"acpi_xgboost_grid_search_results.csv\", index=False)"
   ],
   "id": "6420307183b162eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#print the best parameters\n",
    "best_params = grid_results.iloc[0]\n",
    "print(\"\\nBest Parameters:\")\n",
    "print(best_params)"
   ],
   "id": "9b8c45372ddeb725"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# train and save the best ACPI model\n",
    "best_acpi_model = ACPI(\n",
    "    model_cali=xgb_wrapper,\n",
    "    n_estimators= int(best_params['n_estimators']),\n",
    "    max_depth= int(best_params['max_depth']),\n",
    "    min_node_size= int(best_params['min_node_size'])\n",
    ")\n",
    "best_acpi_model.fit(X_cal, y_cal)\n",
    "best_acpi_model.fit_calibration(X_cal, y_cal, quantile=0.9\n",
    ", only_qrf=True)\n",
    "import joblib\n",
    "joblib.dump(best_acpi_model, \"CF_Generators/models/acpi_xgboost_model_1.pkl\")\n"
   ],
   "id": "910ac752c84e9eaf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3d1ed407c08477ad"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
